# yowo_dataset.py íŒŒì¼

import os
import glob
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np # NumPyë¥¼ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.

class YOWODataset(Dataset):
    def __init__(self, root_dir, split='train', sequence_length=16, transform=None, max_objs=1, img_size=(224, 224)):
        self.sequence_length = sequence_length
        self.transform = transform
        self.max_objs = max_objs
        self.img_size = img_size # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì¶”ê°€
        self.samples = []

        frame_root = os.path.join(root_dir, split, 'frames')
        label_root = os.path.join(root_dir, split, 'labels')

        video_folders = sorted(os.listdir(frame_root))

        for video_name in video_folders:
            frame_dir = os.path.join(frame_root, video_name)
            label_dir = os.path.join(label_root, video_name)

            frame_paths = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))
            
            # ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œëŠ” í”„ë ˆì„ ê²½ë¡œë¡œë¶€í„° ìƒì„± (ë ˆì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„)
            # label_paths = sorted(glob.glob(os.path.join(label_dir, '*.txt')))

            if len(frame_paths) < sequence_length:
                continue

            for i in range(len(frame_paths) - sequence_length + 1):
                frame_seq = frame_paths[i:i + sequence_length]
                
                # ì‹œí€€ìŠ¤ ë‚´ í•˜ë‚˜ë¼ë„ 1ì´ë©´ í–‰ë™ì´ ìˆë‹¤ê³  ê°„ì£¼
                is_abnormal = False
                for frame_path in frame_seq:
                    # ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œë¥¼ í”„ë ˆì„ íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±
                    label_path = frame_path.replace('frames', 'labels').replace('.jpg', '.txt')
                    if os.path.exists(label_path):
                        with open(label_path, 'r') as f:
                            if f.read().strip() == '1':
                                is_abnormal = True
                                break
                
                self.samples.append((frame_seq, is_abnormal))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        # ğŸ’¡ [ìˆ˜ì •] ì „ì²´ __getitem__ í•¨ìˆ˜ë¥¼ ì´ ì½”ë“œë¡œ êµì²´í•˜ì„¸ìš”.
        try:
            frame_seq_paths, is_abnormal = self.samples[idx]
            images = []

            for frame_path in frame_seq_paths:
                img = Image.open(frame_path).convert('RGB')
                
                # ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤ (ëª¨ë“  ì´ë¯¸ì§€ í¬ê¸° í†µì¼)
                img = img.resize(self.img_size)

                if self.transform:
                    img = self.transform(img)
                else:
                    # ğŸ’¡ [ìˆ˜ì • 1] PIL ì´ë¯¸ì§€ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ í…ì„œë¡œ ë§Œë“­ë‹ˆë‹¤.
                    img_np = np.array(img)
                    img = torch.tensor(img_np).permute(2, 0, 1).float() / 255.0
                
                images.append(img)

            # [T, C, H, W]
            video_tensor = torch.stack(images, dim=0)

            # ---------- ë¼ë²¨ ìƒì„± (YOLO í¬ë§·ìš©) ----------
            # [class, x_center, y_center, w, h]
            target = torch.zeros((self.max_objs, 5))
            if is_abnormal:
                # ì „ì²´ í”„ë ˆì„ì„ bboxë¡œ ê°€ì •
                target[0] = torch.tensor([1, 0.5, 0.5, 1.0, 1.0])

            return video_tensor, target

        except Exception as e:
            # ğŸ’¡ [ìˆ˜ì • 2] 'index'ë¥¼ 'idx'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
            print(f"ğŸš¨ [Dataset Error] Corrupted data at index {idx}. Error: {e}. Skipping.")
            
            # ğŸ’¡ [ìˆ˜ì • 3] ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì •ìƒ ë°ì´í„°ì™€ ë™ì¼í•œ ëª¨ì–‘ì˜ 'ë”ë¯¸' í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            dummy_video = torch.zeros((self.sequence_length, 3, self.img_size[1], self.img_size[0]))
            dummy_target = torch.zeros((self.max_objs, 5))
            return dummy_video, dummy_target