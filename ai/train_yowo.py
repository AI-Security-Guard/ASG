import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import multiprocessing

from core.model import YOWO
from core.region_loss import RegionLoss
from yowo_dataset import YOWODataset
from cfg.defaults import get_cfg

# í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ ì •ì˜ëŠ” ë°”ê¹¥ì— ë‘ì–´ë„ ê´œì°®ìŠµë‹ˆë‹¤.

def train(cfg, device, batch_size, num_workers):
    # ------------------ ë°ì´í„° ë¡œë”© ------------------
    print("ğŸ“¦ Loading train dataset...")
    train_dataset = YOWODataset(split="train", root_dir="D:/CCTV/CCTV/yowo_dataset")
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False # num_workers > 0 ì¼ ë•Œ Trueë¡œ ì„¤ì •í•˜ë©´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    )
    print(f"âœ… Train samples: {len(train_dataset)}")

    print("ğŸ“¦ Loading val dataset...")
    val_dataset = YOWODataset(split="val", root_dir="D:/CCTV/CCTV/yowo_dataset")
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True if num_workers > 0 else False
    )
    print(f"âœ… Val samples: {len(val_dataset)}")

    # ------------------ ëª¨ë¸ ë° ì†ì‹¤ í•¨ìˆ˜ ------------------
    model = YOWO(cfg).to(device)
    criterion = RegionLoss(cfg).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    num_epochs = 10

    # ------------------ í•™ìŠµ ë£¨í”„ ------------------
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for batch_idx, (videos, labels) in enumerate(tqdm(train_loader, desc=f"[Epoch {epoch+1}/{num_epochs}] Training")):
            videos, labels = videos.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(videos)
            loss = criterion(outputs, labels, epoch, batch_idx, train_loader)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        print(f"[Epoch {epoch+1}] âœ… Avg Train Loss: {avg_train_loss:.4f}")

        # ------------------ ê²€ì¦ ------------------
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch_idx, (videos, labels) in enumerate(tqdm(val_loader, desc=f"[Epoch {epoch+1}/{num_epochs}] Validation")):
                videos, labels = videos.to(device), labels.to(device)
                outputs = model(videos)
                loss = criterion(outputs, labels, epoch, batch_idx, val_loader)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f"[Epoch {epoch+1}] ğŸ“‰ Avg Val Loss: {avg_val_loss:.4f}")

    # ------------------ ëª¨ë¸ ì €ì¥ ------------------
    torch.save(model.state_dict(), "yowo_final.pth")
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: yowo_final.pth")

# ------------------ ë©”ì¸ ------------------
if __name__ == "__main__":
    multiprocessing.freeze_support()  # Windows ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜ ë°©ì§€ (ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)

    # ------------------ ì„¤ì • ------------------
    cfg = get_cfg()
    cfg.MODEL.NUM_CLASSES = 2
    cfg.MODEL.BACKBONE_2D = "darknet"
    cfg.MODEL.BACKBONE_3D = "resnet18"
    cfg.WEIGHTS.BACKBONE_2D = ""
    cfg.WEIGHTS.BACKBONE_3D = ""
    cfg.WEIGHTS.FREEZE_BACKBONE_2D = False
    cfg.WEIGHTS.FREEZE_BACKBONE_3D = False

    batch_size = 8
    num_workers = 2  # <- ì´ ê°’ì´ 0ë³´ë‹¤ í´ ë•Œ ë¬¸ì œê°€ ë°œìƒí•˜ë¯€ë¡œ êµ¬ì¡° ë³€ê²½ì´ í•„ìˆ˜

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(">>> USING RegionLoss from: core.region_loss")
    train(cfg, device, batch_size, num_workers)